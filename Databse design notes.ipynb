{"cells":[{"source":"# Course Notes\nUse this workspace to take notes, store sample queries, and build your own interactive cheat sheet! \n\n_Note that you need to connect your [SQL cells](https://workspace-docs.datacamp.com/work/sql-cell) to an integration to run a query._\n- _You can use a sample integration from the dropdown menu. This includes the **Course Databases** integration, which contains tables you used in our SQL courses._\n- _You can connect your own integration by following the instructions provided [here](https://workspace-docs.datacamp.com/integrations/what-is-an-integration)._","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"## Take Notes\n\nAdd notes here about the concepts you've learned and SQL cells with code you want to keep.","metadata":{},"id":"a0ac303b-ad44-4690-a9f0-a70619fcabc4","cell_type":"markdown"},{"source":"_Add your notes here_","metadata":{},"id":"540470d7-eb27-442b-b0d5-ab28b0a7da96","cell_type":"markdown"},{"source":"-- A sample query for you to replace!\nSELECT \n    *\nFROM books","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be","executionTime":1721,"lastSuccessfullyExecutedCode":"SELECT \n    *\nFROM books"},"id":"1e1b8b86-a422-405b-a3e1-77ce61ea6bb2","cell_type":"code","execution_count":1,"outputs":[]},{"source":"# 1. Processing, Storing, and Organizing Data","metadata":{},"cell_type":"markdown","id":"b5e68937-2a05-46c9-a7e5-fb401457c29c"},{"source":"## 1.1 OLTP vs OLAP","metadata":{},"cell_type":"markdown","id":"39fce7b8-e802-4f47-8ee4-07530f590ae2"},{"source":"### OLTP - OnLine Transaction Processing\n- OLTP approch is oriented around transactions.\n- They are application-oriented like for book keeping.\n- Data is up to date and operational.\n- Size in gigabytes\n- Queries are simple transactions and frequent update.\n- They are used by more people through out a company and even company's customer.\n- Focus on supporting day to day operations.\n- Keeping track of prices of items.\n- Track all customer transaction, like add, update, delete.\n- Keeping track of emplyees\n### OLAP - OnLine Analytical Processing\n- OLAP approch is oriented around analytics.\n- They are subject oriented, like last quarter book sales\n- Data is consloidated and historical for long term analysis.\n- Size in terabytes.\n- Queries are complex, aggregated and and limited updates.\n- They are typically used by data analystics and data scientists.\n- Tasks are vagure and focus on business decesion making.\n- Analysisng the most profitable item.\n- Analysing most loyal customer\n- Analysing employee of the month","metadata":{},"cell_type":"markdown","id":"431014a3-47a3-48d4-b17a-a3907a52d1b0"},{"source":"## 1.2 Storing data","metadata":{},"cell_type":"markdown","id":"1a6728b9-9a3c-496a-b423-9b1325fec9c4"},{"source":"### Structuring data\n#### 1. Structred data\n- Follows a schema\n- Defined data types and relationships\n- e.g, SQL, tables in relational database\n#### 2. Unstructured data\n- Schemaless\n- Makes up most of the data in the world\n- e.g, photos, chatlogs, MP3\n#### 3. Semistructured data\n- Does not follow large schema\n- self describing structure\n- e.g, NoSQL, XML, JSON","metadata":{},"cell_type":"markdown","id":"25ed7fee-3006-436f-8011-63e907cae049"},{"source":"### Storing data beyond traditional databases\n#### Traditional databases\n- For storing real time relational structured data - OLTP\n#### Data warehouses\n- For analyzing archived structured data - OLAP\n#### Data lakes\n- For storing data of all structures = flexibility and scalability\n- For analyzing big data","metadata":{},"cell_type":"markdown","id":"75a7b4fc-9155-4994-8d8c-69686386d7f1"},{"source":"### Data warehouse\n- Optimized for analytics\n- Organized for reading/aggregating data\n- Usually read only\n- Contains data from multiple sources\n- Massive parallel processing (MPP) for faster queries\n- Typically uses a denormalized schema and dimensional modeling.\n- Amazon redshift, Azure SQL data warehouse and Google Big Query offer data warehouse solutions.\n#### Data marts\n- Subset of data warehouses\n- dedicated to specific topic.\n- Allow departments to have easier access to the data that matters to them.","metadata":{},"cell_type":"markdown","id":"039bfdec-3014-46cf-b62b-593430d0a9ab"},{"source":"### Data lakes\n- Store all types of data at a lower cost. e.g, raw, operational databases, IoT devices logs, real-time, relational and non-relational\n- Retains all data and can take up petabytes (1000 tb).\n- Schema-on-read as opposed to schema-on-write.\n- Need to catalog data otherwise becomes a data swamp\n- Run big data analytics using services such as Apache spark and Hadoop. Useful for deep learning and data discovery because activities require so much data.\n- Google, AWS and microsoft also provide data lake solutions.","metadata":{},"cell_type":"markdown","id":"4b0b0bb4-ab15-4a63-b843-f5c95b47c5da"},{"source":"### ETL vs ELT\n#### ETL\n- It is more traditional approach for warehousing and smaller scale analytics.\n- Data is transformed before loadig into storage, usually to follow the storage's schema\n#### ELT\n- It is more common with big data projects.\n- Data is stored in its native form in a storage solution like a data lake. Portions of data are transformed for different purposes, from building a data warehouse to doing deep learning.","metadata":{},"cell_type":"markdown","id":"c9554e31-6049-4f94-8e2f-724542671046"},{"source":"## 1.3 Database design","metadata":{},"cell_type":"markdown","id":"825fc928-b1b2-44bd-91d8-6dd9342d84cb"},{"source":"### What is database design\n- Determine how data is logically stored. How is data going to be read and updated\n- Uses **databases models** (high level soecification for database structure)\n- \t\tMost popular: relation model\n- \t\tother: NoSQL model, object-oriented model, network model\n- Uses **schemas** (blueprint of database)\n- \t\tDefines tables, fields, relationships, indexes and views\n- \t\tWhen inserted data in relational databases, schemas must be respected.","metadata":{},"cell_type":"markdown","id":"b764cf6f-fcac-4eb3-bda1-d37ec1724a32"},{"source":"## Data modeling\n Process of creating a data model for the data to be stored\n- **Conceptual data model**: describes entities, relationships, andaributes\n- \t\tTools: data structure diagrams, e.g., entity-relational diagrams and UML diagrams\n- **Logical data model**: defines tables, columns, relationships \n- \t\tTools: database models and schemas, e.g., relational model and star schema \n- **Physical data model**: describes physical storage \n- \t\tTools: partitions, CPUs, indexes, backup systems and tablespaces\n","metadata":{},"cell_type":"markdown","id":"c30da5fd-6010-40eb-b5d5-4c0f03d11713"},{"source":"### Dimensional modeling\nAdaptation of the relational model for data warehouse design\n- Optimized for OLAP queries: aggregate data, not updating (OLTP)\n- Built using the star schema\n- Easy to interpret and extend schema\n\n### Elements of dimensional modeling\n**Fact tables** \n- Decided by business use-case \n- Holds records of ametric\n- Changes regularly\n- Connects to dimensions via foreign keys\n**Dimension tables**\n- Holds descriptions of aributes\n- Does not change as often","metadata":{},"cell_type":"markdown","id":"939d68c4-1428-4ec3-8794-fcce17c9ee93"},{"source":"# 2. Database Schemas and Normalization\nlearn to implement star and snowflake schemas, recognize the importance of normalization and see how to normalize databases to different extents.","metadata":{},"cell_type":"markdown","id":"f47b8a5f-f56f-474f-862e-41fb9227c9a7"},{"source":"## 2.1 Star and snowflake schema","metadata":{},"cell_type":"markdown","id":"871bacab-65e7-4be9-8570-3d3a1bd1829b"},{"source":"### Star schema\nIt is the simplest form of dimensional model. Some use the term star schema and dimensional model interchangeably. Star schema is made up of two table **fact** and **dimensional** tables.\n\n#### Fact tables \n- Holds records of a metric that are described further by dimension tables.\n- Changes regularly\n- Connects to dimensions via foreign keys\n#### Dimension tables\n- Holds descriptions of aributes\n- Does not change as often\n\n#### Example\n- Supply book to stores in USA in canada\n- Keep tracks of book sales","metadata":{},"cell_type":"markdown","id":"4b535952-775e-4595-90b5-656f1afa447e"},{"source":"### Star schema example\n![book-star](book-star.png)\n\nExcluding primary and foreign keys, fact table holds sales amount and quantity of books. It's connected to dimension tables with details on the books sold, the time the sales took place and the store buying the books.\n\nEach dimension table represent one-to-many relationship with fact table. e.g, a store can be a part of many book sales, but one sale can only belong to one store.","metadata":{},"cell_type":"markdown","id":"9f147b89-9914-4825-b876-d59165b1df9e"},{"source":"### Snowflake schema\n![snowflake](snowflake.jpg)\nSnowflake schema is an extension of the star schema. The information contain in this schema is the same as the star schema. In fact, the fact table is the same, but the way the dimension tables are structured is different.\n\nStar schema extends one dimension, while the snowflake schema extends over more than one dimension. This is because the dimension tables are **normalized**.","metadata":{},"cell_type":"markdown","id":"94d56c9c-6682-49ff-8853-8e77deab46fd"},{"source":"### Normarlization\n- Database design technique\n- Divides tables into smaller tables and connects them via relationships.\n- **Goal**: To reduce redundancy and increase data integrity.\n**Identify repeating groups of data and create new tables for them**","metadata":{},"cell_type":"markdown","id":"3b31a556-354d-4a9b-a5fa-847e3c785680"},{"source":"## Exercise","metadata":{},"cell_type":"markdown","id":"3c1f8127-d68e-4c2c-8be0-ff841ed48356"},{"source":"**Adding foreign keys**\nForeign key references are essential to both the snowflake and star schema. When creating either of these schemas, correctly setting up the foreign keys is vital because they connect dimensions to the fact table. They also enforce a one-to-many relationship, because unless otherwise specified, a foreign key can appear more than once in a table and primary key can appear only once.\n\nThe fact_booksales table has three foreign keys: book_id, time_id, and store_id. In this exercise, the four tables that make up the star schema below have been loaded. However, the foreign keys still need to be added. \n\n**Instructions**\n- In the constraint called sales_book, set book_id as a foreign key.\n- In the constraint called sales_time, set time_id as a foreign key.\n- In the constraint called sales_store, set store_id as a foreign key.","metadata":{},"cell_type":"markdown","id":"ed717e88-0ca3-40dd-b45c-cd14e4dba38d"},{"source":"-- Add the book_id foreign key\nALTER TABLE fact_booksales ADD CONSTRAINT sales_book\n    FOREIGN KEY (book_id) REFERENCES dim_book_star (book_id);\n    \n-- Add the time_id foreign key\nALTER TABLE fact_booksales ADD CONSTRAINT sales_time\n    FOREIGN KEY (time_id) REFERENCES dim_time_star (time_id);\n    \n-- Add the store_id foreign key\nALTER TABLE fact_booksales ADD CONSTRAINT sales_store\n    FOREIGN KEY (store_id) REFERENCES dim_store_star (store_id)","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"97f3be70-5395-4547-a9de-67e506f4cf3c","execution_count":null,"outputs":[]},{"source":"**Extending the book dimension**\nIn the video, we saw how the book dimension differed between the star and snowflake schema. The star schema's dimension table for books, dim_book_star, has been loaded and below is the snowflake schema of the book dimension\n\nIn this exercise, you are going to extend the star schema to meet part of the snowflake schema's criteria. Specifically, you will create dim_author from the data provided in dim_book_star.\n\n**Instructions** \n- Create dim_author with a column for author.\n- Insert all the distinct authors from dim_book_star into dim_author.","metadata":{},"cell_type":"markdown","id":"8a8f26f6-af04-4dae-91b5-7a9f3583bed3"},{"source":"-- Create dim_author with an author column\nCREATE TABLE dim_author (\n    author varchar(256)  NOT NULL\n);\n\n-- Insert authors into the new table\nINSERT INTO dim_author\nSELECT DISTINCT author FROM dim_book_star;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"a8d1bdee-b061-44a1-b9f8-21554ceff260","execution_count":null,"outputs":[]},{"source":"Instructions \n- Alter dim_author to have a primary key called author_id.\n- Output all the columns of dim_author.","metadata":{},"cell_type":"markdown","id":"781c9f95-e43a-49e8-ac61-efbf215f3589"},{"source":"-- Add a primary key \nALTER TABLE dim_author ADD COLUMN author_id SERIAL PRIMARY KEY;\n\n-- Output the new table\nSELECT * FROM dim_author;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"efd22471-f053-43d2-81a8-1edb32ffab76","execution_count":null,"outputs":[]},{"source":"## 2.2 Normalization","metadata":{},"cell_type":"markdown","id":"4c8cf055-5189-46dc-a1fe-a7da43e78481"},{"source":"### Normalizion\n**Normalization ensures better data integrity**\n- Enforce data consistency\n- \t\t\tMust respect naming conventions because of referntial integrity, e.g., 'California', not 'CA' or 'california'\n- Safer updating, removing, and inserting\n- \t\t\tLess data redundancy = less records to alter\n- Easier to redesign by extending\n- \t\t\tSmaller tables are easier to extend than larger tables","metadata":{},"cell_type":"markdown","id":"90fd8032-9dc5-4e3a-a934-f43084e96be2"},{"source":"### Advantages\n- Eliminates data redundancy: save on storage\n- Better data integrity: accurate and consistent data\n\n### Disadvantages\n- Complex queries equire more CPU\n\n**Deciding between normalization and denormalization comes down to how read or write intensive your database is going to be**\n\n### OLTP\n- Typically high normalized\n- write-intensive\n- Prioritize quicker and safer insertion of data\n\n### OLAP\n- Typically less normalized\n- Read-intensive\n- Priorized quicker queries for analystics","metadata":{},"cell_type":"markdown","id":"f698b72e-9b16-461c-b41a-6301fd4f586c"},{"source":"## Exercise","metadata":{},"cell_type":"markdown","id":"e05b8ade-fa31-469f-afc5-93b69d4d10d2"},{"source":"### Querying the star schema\nThe novel genre hasn't been selling as well as your company predicted. To help remedy this, you've been tasked to run some analytics on the novel genre to find which areas the Sales team should target. To begin, you want to look at the total amount of sales made in each state from books in the novel genre.\n\nLuckily, you've just finished setting up a data warehouse with the following star schema:\n\n**Instructions**\n- Select state from the appropriate table and the total sales_amount.\n- Complete the JOIN on book_id.\n- Complete the JOIN to connect the dim_store_star table\n- Conditionally select for books with the genre novel.\n- Group the results by state.","metadata":{},"cell_type":"markdown","id":"a4e2aeeb-2673-41a9-a64d-28cf6b87d839"},{"source":"SELECT dim_store_star.state, sum(sales_amount)\nFROM fact_booksales\n\t-- Join to get book information\n    JOIN dim_book_star on fact_booksales.book_id = dim_book_star.book_id\n\t-- Join to get store information\n    JOIN dim_store_star on fact_booksales.store_id = dim_store_star.store_id\n-- Get all books with in the novel genre\nWHERE  \n    dim_book_star.genre = 'novel'\n-- Group results by state\nGROUP BY\n    dim_store_star.state;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"2574459c-a124-4b74-aba3-13359784626e","execution_count":null,"outputs":[]},{"source":"### Querying the snowflake schema\nImagine that you didn't have the data warehouse set up. Instead, you'll have to run this query on the company's operational database, which means you'll have to rewrite the previous query with the following snowflake schema:\n\n**Instructions**\n\n- Select state from the appropriate table and the total sales_amount.\n- Complete the two JOINS to get the genre_id's.\n- Complete the three JOINS to get the state_id's.\n- Conditionally select for books with the genre novel.\n- Group the results by state.","metadata":{},"cell_type":"markdown","id":"8f34aa16-2c73-4ced-adbf-d043fc24d38d"},{"source":"-- Output each state and their total sales_amount\nSELECT dim_state_sf.state, sum(sales_amount)\nFROM fact_booksales\n    -- Joins for genre\n    JOIN dim_book_sf on fact_booksales.book_id = dim_book_sf.book_id\n    JOIN dim_genre_sf on dim_book_sf.genre_id = dim_genre_sf.genre_id\n    -- Joins for state \n    JOIN dim_store_sf on fact_booksales.store_id = dim_store_sf.store_id \n    JOIN dim_city_sf on dim_store_sf.city_id = dim_city_sf.city_id\n\tJOIN dim_state_sf on  dim_city_sf.state_id = dim_state_sf.state_id\n-- Get all books with in the novel genre and group the results by state\nWHERE  \n    dim_genre_sf.genre = 'novel'\nGROUP BY\n    dim_state_sf.state;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"bf7284eb-157b-458d-a362-b1e03aed2f2e","execution_count":null,"outputs":[]},{"source":"### Updating countries\nGoing through the company data, you notice there are some inconsistencies in the store addresses. These probably occurred during data entry, where people fill in fields using different naming conventions. This can be especially seen in the country field, and you decide that countries should be represented by their abbreviations. The only countries in the database are Canada and the United States, which should be represented as USA and CA.\n\nIn this exercise, you will compare the records that need to be updated in order to do this task on the star and snowflake schema. dim_store_star and dim_country_sf have been loaded.\n\n**Instructions** \n- Output all the records that need to be updated in the star schema so that countries are represented by their abbreviations","metadata":{},"cell_type":"markdown","id":"1f03546a-64dc-47a6-bef1-622179a0fbd9"},{"source":"SELECT * FROM dim_country_sf \nWHERE country != 'USA' AND country !='CA';","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"5367d1c5-8285-4618-83b2-ca6771908c8c","execution_count":null,"outputs":[]},{"source":"### Extending the snowflake schema\nThe company is thinking about extending their business beyond bookstores in Canada and the US. Particularly, they want to expand to a new continent. In preparation, you decide a continent field is needed when storing the addresses of stores.\n\nLuckily, you have a snowflake schema in this scenario. As we discussed in the video, the snowflake schema is typically faster to extend while ensuring data consistency. Along with dim_country_sf, a table called dim_continent_sf has been loaded. It contains the only continent currently needed, North America, and a primary key. In this exercise, you'll need to extend dim_country_sf to reference dim_continent_sf.\n\n**Instructions**\n- Add a continent_id column to dim_country_sf with a default value of 1. Note thatNOT NULL DEFAULT(1) constrains a value from being null and defaults its value to 1.\n- Make that new column a foreign key reference to dim_continent_sf's continent_id.","metadata":{},"cell_type":"markdown","id":"859e62b7-f8f4-4952-93d1-81b9af542291"},{"source":"-- Add a continent_id column with default value of 1\nALTER TABLE dim_country_sf\nADD continent_id int NOT NULL DEFAULT(1);\n\n-- Add the foreign key constraint\nALTER TABLE dim_country_sf ADD CONSTRAINT country_continent\n   FOREIGN KEY (continent_id) REFERENCES dim_continent_sf(continent_id);\n   \n-- Output updated table\nSELECT * FROM dim_country_sf;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"0b2e6347-751b-49e9-ae85-e30cd4b10ff0","execution_count":null,"outputs":[]},{"source":"## 2.3 Normal Forms","metadata":{},"cell_type":"markdown","id":"2b642d91-7ab4-402b-a3a2-4d20f662bb4c"},{"source":"### 1NF rules\n- Each reord must be unique\n- Each cell must hold one value\n\n### 2NF\n- Must satisfy 1NF and if primary key is one column\n- \t\tthen automatically satifies 2NF\n- If there is a composite primary key\n- \t\tthen each non-key column must be dependent on all keys\n\n### 3NF\n- Satifies 2NF\n- No transitive dependencies: non-key columns can't depend on other non-key columns","metadata":{},"cell_type":"markdown","id":"8dd6a3e2-723f-4d07-bf89-efe448c9c66b"},{"source":"### Data anomalies\n- **Update anomaly**: Data inconsistency caused by data redundancy when updating\n- **Insertion anomaly**: Unable to add record due to missing attributes\n- **Deletion anomaly**: Deletion of records causes unintentional loss of data\n\nThe more normalized the database, the less prone it will be to data anomalies","metadata":{},"cell_type":"markdown","id":"42004921-1e33-42b7-8f18-be0b0ed5469f"},{"source":"## Exercise","metadata":{},"cell_type":"markdown","id":"4234d93b-c4da-446c-886e-6b58cf07ed4f"},{"source":"### Converting to 1NF\nIn the next three exercises, you'll be working through different tables belonging to a car rental company. Your job is to explore different schemas and gradually increase the normalization of these schemas through the different normal forms. At this stage, we're not worried about relocating the data, but rearranging the tables.\n\nA table called customers has been loaded, which holds information about customers and the cars they have rented.\n\n**Instructions** \n- cars_rented holds one or more car_ids and invoice_id holds multiple values. Create a new table to hold individual car_ids and invoice_ids of the customer_ids who've rented those cars.\n- Drop two columns from customers table to satisfy 1NF","metadata":{},"cell_type":"markdown","id":"673e387b-24fd-4b8b-89c9-3bbdcf59e3de"},{"source":"-- Create a new table to hold the cars rented by customers\nCREATE TABLE cust_rentals (\n  customer_id INT NOT NULL,\n  car_id VARCHAR(128) NULL,\n  invoice_id VARCHAR(128) NULL\n);\n\n-- Drop column from customers table to satisfy 1NF\nALTER TABLE customers\nDROP COLUMN cars_rented,\nDROP COLUMN invoice_id;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"0d1d67fa-c324-49a1-bbec-965e27c88b95","execution_count":null,"outputs":[]},{"source":"### Converting to 2NF\nLet's try normalizing a bit more. In the last exercise, you created a table holding customer_ids and car_ids. This has been expanded upon and the resulting table, customer_rentals, has been loaded for you. Since you've got 1NF down, it's time for 2NF.\n\n**Instructions** \n- Create a new table for the non-key columns that were conflicting with 2NF criteria.\n- Drop those non-key columns from customer_rentals.","metadata":{},"cell_type":"markdown","id":"9cc066ac-25c2-46c9-94c7-96c7adeba589"},{"source":"-- Create a new table to satisfy 2NF\nCREATE TABLE cars (\n  car_id VARCHAR(256) NULL,\n  model VARCHAR(128),\n  manufacturer VARCHAR(128),\n  type_car VARCHAR(128),\n  condition VARCHAR(128),\n  color VARCHAR(128)\n);\n\n-- Drop columns in customer_rentals to satisfy 2NF\nALTER TABLE customer_rentals\nDROP COLUMN model,\nDROP COLUMN manufacturer, \nDROP COLUMN type_car,\nDROP COLUMN condition,\nDROP COLUMN color;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"57ab2927-99fb-41ea-8fc8-50c1a35fc406","execution_count":null,"outputs":[]},{"source":"### Converting to 3NF\nLast, but not least, we are at 3NF. In the last exercise, you created a table holding car_idss and car attributes. This has been expanded upon. For example, car_id is now a primary key. The resulting table, rental_cars, has been loaded for you.\n\n**Instructions** \n- Create a new table for the non-key columns that were conflicting with 3NF criteria.\n- Drop those non-key columns from rental_cars.","metadata":{},"cell_type":"markdown","id":"10f4c1bd-fc19-4742-9da3-e755d3998390"},{"source":"-- Create a new table to satisfy 3NF\nCREATE TABLE car_model(\n  model VARCHAR(128),\n  manufacturer VARCHAR(128),\n  type_car VARCHAR(128)\n);\n\n-- Drop columns in rental_cars to satisfy 3NF\nALTER TABLE rental_cars\nDROP COLUMN manufacturer, \nDROP COLUMN type_car;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"fd803920-52d2-415a-b092-c41e96ee76c2","execution_count":null,"outputs":[]},{"source":"","metadata":{},"cell_type":"markdown","id":"892643e3-61d0-481d-ab72-449056251bc5"},{"source":"# 3. Database views","metadata":{},"cell_type":"markdown","id":"1ea5aefb-f6c9-4282-9457-299fb9896446"},{"source":"## 3.1 Database views","metadata":{},"cell_type":"markdown","id":"6e1105a1-b7ca-45de-9c91-b4974eb9eadb"},{"source":"In a databse, view is the result set of a stored query on the data, which the database users can query just as they would in a persistent database collection onject.\n\n**Virtual table that is not part of the physical schema**\n- A view isn't stored in physical memory; instead, the query to create the view is.\n- Data is aggregated from  data in tables\n- Can be queried like a regular database table\n- No need to retype common queries or alter schema.\n\n**Example**<br>\n\tReturn titles and authors of the `science fiction` genre<br>\n**Query**<br>\n`CREATE VIEW scifi_books AS`<br>\n`SELECT title, author, genre`<br>\n`FROM dim_book_sf`<br>\n`JOIN dim_genre_sf ON dim_book_sf.genre_id = dim_genre_sf.genre_id`<br>\n`JOIN dim_author_sf ON dim_book_sf.author.id = dim_author_sf.author.id`<br>\n`WHERE`<br>\n`dim_genre_sf.genre = 'science fiction';`<br>\n\nTo query a view - `SELECT * FROM scifi_books`<br>\nTo check all the views in a database (postgres, includes system views) - `SELECT * FROM INFORMATION_SCHEMA.views`<br>\nTo exclude system views run this command - `SELECT * FROM INFORMATION_SCHEMA.views WHERE table_schema NOT IN ('pg_catalog', 'infrmation_schema')`\n\n### Benefits of views\n- Doesn't take up storage\n- A form of access control\n- \tHide sensitive columns and restrict what user can see\n- Masks complexity of queries\n- \tUseful for highly normalized shemas","metadata":{},"cell_type":"markdown","id":"c70daa7d-31e2-499b-bac5-ae217478dcd3"},{"source":"### Example","metadata":{},"cell_type":"markdown","id":"fb9f35d8-27c3-44bf-9bd0-9970e283b721"},{"source":"### Viewing views\nBecause views are very useful, it's common to end up with many of them in your database. It's important to keep track of them so that database users know what is available to them.\n\nThe goal of this exercise is to get familiar with viewing views within a database and interpreting their purpose. This is a skill needed when writing database documentation or organizing views.\n\n**Instructions** \n- Query the information schema to get views.\n- Exclude system views in the results.","metadata":{},"cell_type":"markdown","id":"50b1167b-89c8-4578-9775-6caa3fc3dd22"},{"source":"-- Get all non-systems views\nSELECT * FROM INFORMATION_SCHEMA.views\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema');","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"54b88cc1-213f-447e-b846-dc1d6bb315a5","execution_count":null,"outputs":[]},{"source":"### Creating and querying a view\nHave you ever found yourself running the same query over and over again? Maybe, you used to keep a text copy of the query in your desktop notes app, but that was all before you knew about views!\n\nIn these Pitchfork reviews, we're particularly interested in high-scoring reviews and if there's a common thread between the works that get high scores. In this exercise, you'll make a view to help with this analysis so that we don't have to type out the same query often to get these high-scoring reviews.\n\n**Instruction**\n- Create a view called high_scores that holds reviews with scores above a 9.","metadata":{},"cell_type":"markdown","id":"cbb33157-e2c1-4aee-bac6-2ad49f77268e"},{"source":"CREATE VIEW high_scores AS\nSELECT * FROM REVIEWS\nWHERE score > 9;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"d1133b03-924e-464e-8628-409b8ed87b18","execution_count":null,"outputs":[]},{"source":"**Instruction**\n- Count the number of records in high_scores that are self-released in the label field of the labels table.","metadata":{},"cell_type":"markdown","id":"76295abd-88d7-4a7e-8360-cca55a2bf67a"},{"source":"-- Create a view for reviews with a score above 9\nCREATE VIEW high_scores AS\nSELECT * FROM REVIEWS\nWHERE score > 9;\n\n-- Count the number of self-released works in high_scores\nSELECT COUNT(*) FROM high_scores\nINNER JOIN labels ON high_scores.reviewid = labels.reviewid\nWHERE labels.label = 'self-released';","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"c473862a-5869-4502-99e0-6bf59770fd11","execution_count":null,"outputs":[]},{"source":"## 3.2 Managing views","metadata":{},"cell_type":"markdown","id":"f0170667-dfa6-4584-897f-8dffd53d341b"},{"source":"### Granting and revoking access to a view\nTo give and remove user permissions, we use the SQL `GRANT` and `REVOKE` command.\n- **Privilages**: `SELECT`, `INSERT`, `UPDATE`, `DELETE`, etc\n- **Objects**: table, view, schema, etc\n- **Roles**: a database user or a group of database users.\n\nTo write a query<br>\n`GRANT privilage(s) pr REVOKE privilage(s)`<br>\n`ON object`<br>\n`TO role` or `FROM role`","metadata":{},"cell_type":"markdown","id":"9765bcdb-ddf9-4e11-9c7b-72284f07c74e"},{"source":"### Updating a view\nWhen you a update command is run it updates the tables<br>\n**Not all views are updatable** - Depend on type of SQL being used.<br>\n- View is made up of one table\n- Doesn't use a window or aggregate function\n\n### Inserting into a view\nWhen an insert command runs into a view, it inserts into the table behind it. Its usually a good idea to use views for read only purposes.<br>\n**Not all view are insertable: Avoid modifying data through views**\n\n### Dropping a view\nDropping a view is straightforward with the DROP command. There are two useful parametes to know about: `CASCADE` and `RESTRICT`.Sometimes, there are SQL objects that are depend on views.\n- `RESTRICT`(default): returns an error if there are objects that depend on the view.\n- `CASCADE`: drops view and any object that depends on that view.\n\n### Redefining a view\n`CREATE OR REPLACE VIEW view_name AS new_query`<br>\nIf a view with `view_name` exists, it is replaced\n- `new_query` must generate the same column name, order, and data types as the old query.\n- The column output may be different\n- New columns may be added at the end.\n**If these criteria can't be met drop the existing view and crate a new one**\n","metadata":{},"cell_type":"markdown","id":"40aabfe9-b16f-4b6d-8e06-cdf1aa41ff03"},{"source":"### Exercise","metadata":{},"cell_type":"markdown","id":"a41633e5-eb33-47c8-b573-50b045d792b4"},{"source":"**Creating a view from other views**\nViews can be created from queries that include other views. This is useful when you have a complex schema, potentially due to normalization, because it helps reduce the JOINS needed. The biggest concern is keeping track of dependencies, specifically how any modifying or dropping of a view may affect other views.\n\nIn the next few exercises, we'll continue using the Pitchfork reviews data. There are two views of interest in this exercise. top_15_2017 holds the top 15 highest scored reviews published in 2017 with columns reviewid,title, and score. artist_title returns a list of all reviewed titles and their respective artists with columns reviewid, title, and artist. From these views, we want to create a new view that gets the highest scoring artists of 2017.\n\n**Instruction**\n- Create a view called top_artists_2017 with artist from artist_title.\n- To only return the highest scoring artists of 2017, join the views top_15_2017 and artist_title on reviewid.\n- Output top_artists_2017.","metadata":{},"cell_type":"markdown","id":"1e9920ea-b379-43c5-8d2e-cce14572faca"},{"source":"-- Create a view with the top artists in 2017\nCREATE VIEW top_artists_2017 AS\n-- with only one column holding the artist field\nSELECT artist_title.artist FROM artist_title\nINNER JOIN top_15_2017\nON artist_title.reviewid = top_15_2017.reviewid;\n\n-- Output the new view\nSELECT * FROM top_artists_2017;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"38a0e581-1d6b-494e-9970-2b811454c8ac","execution_count":null,"outputs":[]},{"source":"**Granting and revoking access**\nAccess control is a key aspect of database management. Not all database users have the same needs and goals, from analysts, clerks, data scientists, to data engineers. As a general rule of thumb, write access should never be the default and only be given when necessary.\n\nIn the case of our Pitchfork reviews, we don't want all database users to be able to write into the long_reviews view. Instead, the editor should be the only user able to edit this view.\n\n**Instruction**\n- Revoke all database users' update and insert privileges on the long_reviews view.\n- Grant the editor user update and insert privileges on the long_reviews view.","metadata":{},"cell_type":"markdown","id":"f4abfc71-bfc3-4737-a28f-ebfef963c77e"},{"source":"-- Revoke everyone's update and insert privileges\nREVOKE UPDATE, INSERT ON long_reviews FROM PUBLIC; \n\n-- Grant the editor update and insert privileges \nGRANT UPDATE, INSERT ON long_reviews TO editor; ","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"d9f73715-0063-4a16-8ce2-5ce891e5d9d6","execution_count":null,"outputs":[]},{"source":"**Redefining a view**\nUnlike inserting and updating, redefining a view doesn't mean modifying the actual data a view holds. Rather, it means modifying the underlying query that makes the view. In the last video, we learned of two ways to redefine a view: (1) CREATE OR REPLACE and (2) DROP then CREATE. CREATE OR REPLACE can only be used under certain conditions.\n\nThe artist_title view needs to be appended to include a column for the label field from the labels table.\n\n**Instruction**\n- Use CREATE OR REPLACE to redefine the artist_title view.\n- Respecting artist_title's original columns of reviewid, title, and artist, add a label column from the labels table.\n- Join the labels table using the reviewid field.","metadata":{},"cell_type":"markdown","id":"6270f028-6abb-4176-baec-714153b40e3a"},{"source":"-- Redefine the artist_title view to have a label column\nCREATE OR REPLACE VIEW artist_title AS\nSELECT reviews.reviewid, reviews.title, artists.artist, labels.label\nFROM reviews\nINNER JOIN artists\nON artists.reviewid = reviews.reviewid\nINNER JOIN labels\nON reviews.reviewid = labels.reviewid;\n\nSELECT * FROM artist_title;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"d62f1c49-21d5-49ab-8274-7611ded4235d","execution_count":null,"outputs":[]},{"source":"## 3.3 Materialized views","metadata":{},"cell_type":"markdown","id":"6de76f4e-961c-458a-ba5b-d6d33dc7e96f"},{"source":"### Types of views\n**Non-materialized views**<br>\nIn the all previous section we have read about non-materialized views.\n\n**Materialized Views**<br>\n- Physically materialized\n- Stores the query results, not the query\n- Querying a materialized view means accessing the stored query results\n- \tNot running the query like a non-materialized view\n- Refreshed or rematerialized when prompted or scheduled.\n\n### When to use materialized views\n- Queries with long execution time.\n- Don't use on data that is being updated often, because then analysis will be run too often on out-of-date data.\n- Typically use for OLAP (Data warehouses), more for analysis than writing to data.\n \n### Implementing materialized views (PostgreSQL)\n`CREATE MATERIALIZED VIEW my_mv AS SELECT * FROM existing_table;`\n\nFor refreshing views in PostgreSQL **cron jobs** is used\n\n### Managing dependencies\n- Materialized views often depend on other materialized views\n- Create a **dependency chain** when refreshing views\n- Not the most efficient to refresh all views at the same time\n\n### Tools for managing dependencies\n- Use Directed Acyclic Graphs (DAGs) to keep track of views\n- Pipeline schedular tools\n- Examples: Airflow, Luigi","metadata":{},"cell_type":"markdown","id":"54f07bbe-95b4-41d2-aa69-f1d3f483184a"},{"source":"","metadata":{},"cell_type":"markdown","id":"4c703534-9b28-4cda-b03c-1499f473760a"},{"source":"### Exercise","metadata":{},"cell_type":"markdown","id":"a706f69c-3be8-4fe5-9e1a-6a3d97064bfa"},{"source":"**Creating and refreshing a materialized view**\nThe syntax for creating materialized and non-materialized views are quite similar because they are both defined by a query. One key difference is that we can refresh materialized views, while no such concept exists for non-materialized views. It's important to know how to refresh a materialized view, otherwise the view will remain a snapshot of the time the view was created.\n\nIn this exercise, you will create a materialized view from the table genres. A new record will then be inserted into genres. To make sure the view has the latest data, it will have to be refreshed.\n\n**Instructions**\n\n- Create a materialized view called genre_count that holds the number of reviews for each genre.\n- Refresh genre_count so that the view is up-to-date.","metadata":{},"cell_type":"markdown","id":"2bd43481-4c2f-45cf-89b1-a82972221269"},{"source":"-- Create a materialized view called genre_count \nCREATE MATERIALIZED VIEW genre_count AS\nSELECT genre, COUNT(*) \nFROM genres\nGROUP BY genre;\n\nINSERT INTO genres\nVALUES (50000, 'classical');\n\n-- Refresh genre_count\nREFRESH MATERIALIZED VIEW genre_count;\n\nSELECT * FROM genre_count;","metadata":{"customType":"sql","dataFrameVariableName":"df","initial":false,"integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"0e75e0e7-8773-4f43-90c8-dcbd9501f27c","execution_count":null,"outputs":[]},{"source":"# 4 Database roles and access controls","metadata":{},"cell_type":"markdown","id":"afb89d26-9877-45d1-96c7-697e472124a7"},{"source":"## 4.1 Database roles","metadata":{},"cell_type":"markdown","id":"dff11017-d460-41eb-b9b6-35f17e7b9ae9"},{"source":"### Database roles\n- Roles are use to manage database access permissions\n- A database role is an entity that contains information that:\n    - \tDefine roles privileges\n        - Can you login?\n        - can you create databases?\n        - Can you write tables?\n    - \tInteract with the client authentication system\n    \t- What the role's password is?\n- Roles can be assigned to one or mere users\n- Roles are global, you can reference roles across all individual databases in your cluster\n \n ### Create a role\n- Empty role\n\t- `CREATE ROLE ata_analyst;` What the data_analyst role can do is currently empty.\n- Roles with some attributes set\n\t- `CREATE ROLE intern WITH PASSWORD 'InternPassword' VALID UNTIL '2020-01-01';`. Intern role, specifying the password attribute and valid until date attribute.\n\t- `CREATE ROLE admin CREATEDB;`. Admin role with ability to create databases.\n\t- `ALTER ROLE admin CREATEROLE`. To change an attribute for an already created role, now admin can create roles too.\n\n### GRANT and REVOKE privileges\nTo grant and revoke specific access control privileges on objects, like tables, views and schemas, you use GRANT and REVOKE.<br>\n`GRANT UPDATE ON ratings TO data_analyst;`. Data analyst can be able to update the ratings table.<br>\n`REVOKE UPDATE ON ratings FROM data_analyst;`. Don't need it anymore.<br>\n- The available privileges in PostgreSQL are:\n\t- `SELECT, INSERT, UPDATE, DELETE, TRUNCATE, REFERENCES, TRIGGER, CREATE, CONNECT, TEMPORARY, EXECUTE,` and `USAGE`.\n\n### Users and groups\nA role is an entity that can function as a user and/or a group.\n- Users roles\n\t- `CREATE ROLE alex WITH PASSWORD 'InternPassword' VALID UNTIL '2020-01-01';`. Giving a role to specific intern alex\n- Group roles\n\t- `CREATE ROLE data_analyst;` Giving a role to all the data analysts.\n- In postgreSQL, to add the user role to group role you can do is\n\t- `GRANT data_analyst TO alex;`. Alex can do data analyst work now.\n\t- `REVOKE data_analyst FROM alex;`. ALex no longer is member of data analyst\n\n### Benefits and pitfalls of roles\n**Benefits**<br>\n- Roles live on after usres are deleted\n- Roles can be created before user account\n- By grouping together common access levels, database administrators save time.\n**Pitfalls**\n- Sometimes a roles gives a specific user too much access\n\t- You need to pay attention","metadata":{},"cell_type":"markdown","id":"9fe73916-d8ad-4124-b7b2-364e7adccb29"},{"source":"## Exercise","metadata":{},"cell_type":"markdown","id":"db5fa8b1-3312-4d23-9ae8-ea803403d81a"},{"source":"**Create a role**<br>\nA database role is an entity that contains information that define the role's privileges and interact with the client authentication system. Roles allow you to give different people (and often groups of people) that interact with your data different levels of access.\n\nImagine you founded a startup. You are about to hire a group of data scientists. You also hired someone named Marta who needs to be able to login to your database. You're also about to hire a database administrator. In this exercise, you will create these roles.\n\n**Instructions**<br>\nCreate a role called data_scientist.","metadata":{},"cell_type":"markdown","id":"c5bbfd52-ad02-4066-ba8a-5fa9459fa15a"},{"source":"CREATE ROLE data_scientist;","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"c5b335f4-39d0-4871-86ce-5d7fcafbc98b","execution_count":null,"outputs":[]},{"source":"**Instructions**<br>\nCreate a role called marta that has one attribute: the ability to login (LOGIN).","metadata":{},"cell_type":"markdown","id":"f21fada7-da85-4c65-8fed-875f9d97ad65"},{"source":"-- Create a role for Marta\nCREATE ROLE marta LOGIN;","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"959a2060-1232-484f-9dd8-e79e844fff65","execution_count":null,"outputs":[]},{"source":"**Instructions**<br>\nCreate a role called admin with the ability to create databases (CREATEDB) and to create roles (CREATEROLE).","metadata":{},"cell_type":"markdown","id":"f014072d-b296-4c5f-bc9a-928c82777272"},{"source":"-- Create an admin role\nCREATE ROLE admin WITH CREATEDB CREATEROLE;","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"35161cb5-789c-4040-808a-0f82cae8fd08","execution_count":null,"outputs":[]},{"source":"**GRANT privileges and ALTER attributes**<br>\nOnce roles are created, you grant them specific access control privileges on objects, like tables and views. Common privileges being SELECT, INSERT, UPDATE, etc.\n\nImagine you're a cofounder of that startup and you want all of your data scientists to be able to update and insert data in the long_reviews view. In this exercise, you will enable those soon-to-be-hired data scientists by granting their role (data_scientist) those privileges. Also, you'll give Marta's role a password.\n\n**Instructions**<br>\n- Grant the data_scientist role update and insert privileges on the long_reviews view.\n- Alter Marta's role to give her the provided password.","metadata":{},"cell_type":"markdown","id":"fc627794-b45b-460f-8d95-597d82310bcc"},{"source":"-- Grant data_scientist update and insert privileges\nGRANT UPDATE, INSERT ON login_reviews TO data_scientist;\n\n-- Give Marta's role a password\nALTER ROLE marta WITH PASSWORD 's3cur3p@ssw0rd';","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"a9e3055f-8d05-4fc4-8967-ce38002e09cc","execution_count":null,"outputs":[]},{"source":"**Add a user role to a group role**<br>\nThere are two types of roles: user roles and group roles. By assigning a user role to a group role, a database administrator can add complicated levels of access to their databases with one simple command.\n\nFor your startup, your search for data scientist hires is taking longer than expected. Fortunately, it turns out that Marta, your recent hire, has previous data science experience and she's willing to chip in the interim. In this exercise, you'll add Marta's user role to the data scientist group role. You'll then remove her after you complete your hiring process.\n\n**Instructions**<br>\n- Add Marta's user role to the data scientist group role.\n- Celebrate! You hired multiple data scientists.\n- Remove Marta's user role from the data scientist group role.","metadata":{},"cell_type":"markdown","id":"173def16-487e-4a12-b4cb-b2dda188e663"},{"source":"-- Add Marta to the data scientist group\nGRANT data_scientist TO marta;\n\n-- Celebrate! You hired data scientists.\n\n-- Remove Marta from the data scientist group\nREVOKE data_scientist FROM marta;","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"6686b990-054d-4cb5-a64d-15e1f7f23d9c","execution_count":null,"outputs":[]},{"source":"## 4.2 Table partioning","metadata":{},"cell_type":"markdown","id":"fad5c2af-8bc8-4363-b61b-e2264e15b0e0"},{"source":"### Why partition?\nWhen tables grow (100 Gb/Tb) \n- **Problem:** queries/updates become slower.\n- **Because:** e.g., indices don't fit memory.\n- **Solution:** split table into smaller parts (= partitioning)\nPartitioning is part of physical data model, we distribute the data over several physical entities.\n\n### Vertical partitioning\n\nAs ou saw in the second chapter that to normalize a table, you can use foreign keys and create new tables. As, it can help to reduce redundant data.<br>\nVertical partitioning goes one step further and splits up a table vertically by its columns, wvwn whwn it's already fully normalized.\n\n**Example**<br>\nA table containing product data `name`, `short_description`, `price` and `long_description`. After vertical partitioning, you can end up with two tables: one for the first three columns, and another for the last column.<br>\nWe can link them through a shared key. Let's say the fourth column, containing a long description, is retreived very rarely. We could store the second table on a slower medium. Doing his would improve query time for the first table, as we need to scan less data for search queris.\n\n### Horizontal partitioning\nInstead of splitting tables up over the columns, you can also split up tables over the rows.<br>\n\n**Example**<br>\nLet's say you have a tables where every row is a book sale. You could decide to partition the table according to the timestamp.You could create partitions according to the timestamp, and partition them b quarter.<br>\nDifferent SQL dialects have different ways of creating partitioned tables. We'll look at PostgreSQL, where you can use something called declarative partitioning since PSQL 10.<br>\n`CREATE TABLE sales (`<br>\n\t...<br>\n\t`timestamp DATE NOT NULL`<br>\n`)`<br>\n`PARTITION BY RANGE (timestamp);`<br>\nFirst, you add the `PARTITION BY ` clause to your table creation statement.<br>\nYou pass the column you want to partition by, `'timestamp'` in our case.<BR>\nFinally, it's advised to add an index to column you used for partitioning.\n`CREATE TABLE sales_2019_q1 PARTITION OF sales`<br>\n\t`FOR VALUES FROM ('2019-09-01') TO (2019-03-31);`<br>\n...<br>\n`CREATE TABLE sales_2019_q1 PARTITION OF sales`<br>\n`FOR VALUES FROM ('2019-09-01') TO (2019-03-31);`<br>\n`CREATE INDEX ON sales ('timestamp');`<br>\n    \n### Pros and cons of horizontal partitioning\n**Pros**<br>\n- Indices of heavily-used-partitions fit in memory\n- Move to specific medium: slower vs faster\n- Used for both OLAP and OLTP.\n**Cons**<br>\n- Partitioning existing table can be a hassle, have to create a new table and copy over the data.\n- Can not set same type of constraints on a partitioned table. e.g., the `PRIMARY KEY` constarint.\n\n### Relation to sharding\nWe can take partitioning one step further and distribute the partitions over several machines.<br>\nWehn horizontal partitioning is applied to spread a tabble over several machines, it's called sharding.<br>\nYou can see how this relates to massively parallel processing databases, where each node, or machine, can do calculations on specific shards.","metadata":{},"cell_type":"markdown","id":"e09fb47a-c789-4454-8cdf-1971f08451ea"},{"source":"**Creating vertical partitions**\n\nIn the video, you learned about vertical partitioning and saw an example.\n\nFor vertical partitioning, there is no specific syntax in PostgreSQL. You have to create a new table with particular columns and copy the data there. Afterward, you can drop the columns you want in the separate partition. If you need to access the full table, you can do so by using a JOIN clause.\n\nIn this exercise and the next one, you'll be working with the example database called pagila. It's a database that is often used to showcase PostgreSQL features. The database contains several tables. We'll be working with the film table. In this exercise, we'll use the following columns:\n\n- `film_id`: the unique identifier of the film\n- long_description: a lengthy description of the film\n\n**Instructions**<br>\n- Create a new table `film_descriptions` containing 2 fields: `film_id`, which is of type `INT`, and `long_description`, which is of type `TEXT`.\n- Occupy the new table with values from the `film` table.","metadata":{},"cell_type":"markdown","id":"fd6d696c-683a-469c-9958-39a429be8bbd"},{"source":"-- Create a new table called film_descriptions\nCREATE TABLE film_descriptions (\n    film_id INT,\n    long_description TEXT\n);\n\n-- Copy the descriptions from the film table\nINSERT INTO film_descriptions\nSELECT film_id, long_description FROM film;","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"9bcb6689-b2e6-443b-8592-2000a0fb45eb","execution_count":null,"outputs":[]},{"source":"**Instructions**<br>\n- Drop the field `long_description` from the `film` table.\n- Join the two resulting tables to view the original table.","metadata":{},"cell_type":"markdown","id":"a90fe661-70ab-4b22-8701-ab0b1185dd5d"},{"source":"-- Create a new table called film_descriptions\nCREATE TABLE film_descriptions (\n    film_id INT,\n    long_description TEXT\n);\n\n-- Copy the descriptions from the film table\nINSERT INTO film_descriptions\nSELECT film_id, long_description FROM film;\n    \n-- Drop the descriptions from the original table\nALTER TABLE film DROP COLUMN long_description;\n\n-- Join to view the original table\nSELECT * FROM film \nJOIN film_descriptions USING(film_id);","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"004614ae-081f-47fc-a260-d01dd2dfc218","execution_count":null,"outputs":[]},{"source":"**Creating horizontal partitions**<br>\nIn the video, you also learned about horizontal partitioning.\n\nThe example of horizontal partitioning showed the syntax necessary to create horizontal partitions in PostgreSQL. If you need a reminder, you can have a look at the slides.\n\nIn this exercise, however, you'll be using a list partition instead of a range partition. For list partitions, you form partitions by checking whether the partition key is in a list of values or not.\n\nTo do this, we partition by LIST instead of RANGE. When creating the partitions, you should check if the values are IN a list of values.\n\nWe'll be using the following columns in this exercise:\n\n- `film_id`: the unique identifier of the film\n- `title`: the title of the film\n- `release_year`: the year it's released\n\n**Instructions**<br>\n- Create the table `film_partitioned`, partitioned on the field `release_year`.","metadata":{},"cell_type":"markdown","id":"65757c1b-677d-488a-9d96-6aca50f908b4"},{"source":"-- Create a new table called film_partitioned\nCREATE TABLE film_partitioned (\n  film_id INT,\n  title TEXT NOT NULL,\n  release_year TEXT\n)\nPARTITION BY RANGE (release_year);","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"fc089b61-2b71-418f-a22d-9bab2966f381","execution_count":null,"outputs":[]},{"source":"**Instructions**<br>\n- Create three partitions: one for each release year: `2017`, `2018`, and `2019`. Call the partition for `2019` `film_2019`, etc.","metadata":{},"cell_type":"markdown","id":"133172c4-ecbf-4fda-b8f4-e5cb5adbb123"},{"source":"-- Create a new table called film_partitioned\nCREATE TABLE film_partitioned (\n  film_id INT,\n  title TEXT NOT NULL,\n  release_year TEXT\n)\nPARTITION BY LIST (release_year);\n\n-- Create the partitions for 2019, 2018, and 2017\nCREATE TABLE film_2019\n\tPARTITION OF film_partitioned FOR VALUES IN ('2019');\n    \nCREATE TABLE film_2018\n\tPARTITION OF film_partitioned FOR VALUES IN ('2018');\n    \nCREATE TABLE film_2017\n\tPARTITION OF film_partitioned FOR VALUES IN ('2017');","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"48c5039b-6cef-47f6-b741-acbddbbeb463","execution_count":null,"outputs":[]},{"source":"**Instructions**<br>\n- Occupy the new table, `film_partitioned`, with the three fields required from the `film` table.","metadata":{},"cell_type":"markdown","id":"c7835364-ff33-484a-bb81-28d840aaa860"},{"source":"-- Insert the data into film_partitioned\nINSERT INTO film_partitioned\nSELECT film_id, title, release_year FROM film;\n\n-- View film_partitioned\nSELECT * FROM film_partitioned;","metadata":{"customType":"sql","dataFrameVariableName":"df","integrationId":"c632441c-e1dc-4637-a56d-10b85efd89be"},"cell_type":"code","id":"58f5cf9e-6070-48f6-969a-e7e23223738d","execution_count":null,"outputs":[]},{"source":"## 4.3 Data Integration","metadata":{},"cell_type":"markdown","id":"9ee27bf2-d7f0-4b6e-a13a-8376c43fb280"},{"source":"### Data integration\nWhat if your data is spread accross different databases, formats, schemas and technologies? That's where data integration comes into play.\n\nData integration combines darta from different sources, formats, technologies to provide users with a translated and unified view of that data.\n\n#### Business case examples\n- A company could want a 360 -degree customer view, to see all information departments have about a customer in a unified place.\n- One company acquiring another, and need to combine their respective databases.\n- Legacy systems are also a common case of data integration. An insurance company with claims in old and new systems, would need to integrate daa to query all claims at once.\n\nThere are few things to cinsider when integrating data.\n- **Unified data model**\n\t- Unfied data model could be used to create dashboards, like graphs of daily sales, or data products, such as a recommendation engine. The final data model nedds to be fast enough for your use-case.\n- **Data sources**\n\t- The necessary information is held in the data sources.\n- **Data sources format**\n\t- In Which format is each data stored? It could be PostgreSQL, MongoDB or a csv.\n- **Unifeid data model format**\n\t- Which format should the unified data model take? For example, Redshift, a data warehouse servise offered by AWS\n\n#### Example: DataCamp\nSay DataCamp is launching a skill assessment module. Marketing wants to know which customers to target. They need information from sales (stored in PostgreSQL), to see which customers can afford the new product. They also need information from product department (stored in MongoDB) to identify potential early adopters.<br>\n**Update cadence**<br>\nHow often do you want to update the data? Updating daily would probably sufficient for sales data. For ascenario like air traffic, you want real time updates. Your data sources can have different update cadences.<br>\n\nNow how to assemble the sources which are different formats.\n### Transformations\nAtransformation is a program that extracts content from the table and transform it into chosen format for the unified model. These transformations can be hand coded, but you have to make and maintain a transformation for each data source. You can alse use the data integration tool, which provides the needed ETL like, Apache Airflow or Scriptella.\n\n### Choosing a data integration tool\n- Flexible enough to connect to all of your data sources.\n- Reliable, so it can maintained in a year.\n- Scalable, anticipat an increase in data volume and sources.\n\n#### Automated testing and proactive alerts\nYou should have automated testing and proactive alerts. If any data gets corrupted on its way to the unified data model, the system lets you know. For example, you should aggregate sales data after each transformation and ensure that the total amount remains the same.\n#### Security\nSecurity is also concern: if data access was originally restricted, it should remain restricted in the unified data model. For example, business analyst using the unified data model should not have access to the credit card numbers. You should anonymize the data during ETL so that analysts can only access the first four numbers, to identify the type of card being used.\n#### Data governance\nYou need to conside lineage: for effective auditing, you should know where the data originated and where it is used at all times.","metadata":{},"cell_type":"markdown","id":"e88ec5fc-87b0-4054-90b3-ac300ef4bc0d"},{"source":"## 4.4 Picking a DBMS","metadata":{},"cell_type":"markdown","id":"673cd71b-0cb7-4567-9dc8-b011b5cb87e2"},{"source":"### DBMS\n- DBMS stands for DataBase Management System\n- Create and maintain databases\n    - Data\n    - Database schema - Defines the database's logical structure\n    - Database engine - Allows data to be accessed, locked and modified.\nEssentially, the DBMS serves as an interface between the database and end users or application programs.\n\n### DBMS types\n- Choice of DBMS depends on database type\n- Two types\n    - SQL DBMS\n    - NoSQL DBMS\n\n### SQL DBMS\n- Relational Database Management System (RDBMS)\n- Based on the relational model of data\n- Query language: SQL\n- Some examples include SQL server, PostgreSQL and Oracle SQL\n- Best option to use when:\n    - Data is structured and unchanging\n    - Data must be consistent without leaving room for error\n\n### NoSQL DBMS\n- Less structured\n- Document-cenyered rather than table-centered\n- Data doesn't have to fit into well-defined rows and columns\n- Best option when:\n    - Rapid growth\n    - No clear schema definitions\n    - Large quantities of data\n- Types: key-value store, document store, columnar database, graph database\n\n#### Key-value store\n- Combinition of keys and values \n    - key: unique identifier\n    - value: anything\n- Use case: managing the shopping cart for an online buyer\n- Example: Redis\n\n#### Document store\n- Similar to key-value\n- Values (documents) are stored\n- Use case: content management\n- Example: MongoDB\n\n#### Columnar database\n- Store each column in a separate file \n- Scalable and faster faster at scale\n- Use case: big data analytics where speed is important\n- Example: Cassandra\n\n#### Graph database\n- Data is interconnected and best represented as graph\n- Use case: social media data, recommendations\n- Example: neo4j\n\nThe choice of the database dependa on the business need. If your application has a fixed structure and doesn't need frequent modifications, a SQL DBMS is preferable.<br>\nConversely, if you have applications where data is changing frequently and growing rapidly, like in big data analystics, NoSQL is the best option.","metadata":{},"cell_type":"markdown","id":"15b5812c-e0c4-4046-8e23-589d439f310e"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}